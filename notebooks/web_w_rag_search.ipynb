{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from crewai_tools.tasks import router_task, retriever_task, grader_task, hallucination_task, answer_task\n",
    "# from crewai_tools.tools import router_tool, rag_tool\n",
    "from crewai_tools.agents import router_agent, retriever_agent, grader_agent, hallucination_grader, answer_grader\n",
    "from crewai_tools.crews import rag_crew\n",
    "from crewai_tools.crews import crew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TAVILY_RESULTS=5\n",
    "search_tool = TavilySearchAPIRetriever(\n",
    "    k=TAVILY_RESULTS\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs ={\"question\":\"How does self-attention mechanism help large language models?\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = rag_crew.kickoff(inputs=inputs)\n",
    "result"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
